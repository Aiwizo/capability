# Introduction
The purpose of this repository is to show that we have the skills (both in machine learning and programming) to develop and implement valuable algorithms. Below, you can read about some of our projects and find links to relevant code-snippets.

In machine learning, simplicity is desirable, and the cost of complexity often outweighs the added value. Hopefully, you will notice that we incorporate that mindset into our work.

### Technologies

We use PyTorch or Tensorflow to develop algorithms, but we prefer PyTorch. In the experimentation phase, we use [guild](https://github.com/guildai/guildai) to keep track of each experiment. Guild gives us many tools to compare runs and makes it easy to select the best algorithm.

To make sure that you can use the algorithm in your systems, we create a lightweight and easy to deploy microservice.

# Projects

### Railroad inspection

In collaboration with two companies working with railroad maintenance, we are developing algorithms for identifying critical infrastructure and anomalies in images taken with cameras mounted on trains.

One algorithm is responsible for detecting clamps on contact wires. Itâ€™s a simple object detection problem, but there are some challenges:

- 99% of the images have no clamps, and the class distribution is unbalanced
- The algorithm has to analyze a massive amount of data, so the speed is critical
- There are multiple cameras, and the images are overlapping

To handle the unbalanced dataset, we use stratified sampling based on class and sample within each strata based on the previous loss. The algorithm has a [specialized architecture](https://github.com/Aiwizo/capability/blob/master/railroad_inspection/architecture.py) to make it fast, to support multiple outputs, and enforce coherent behavior. One such architectural detail is the use of a common response map for both location and probability of detection.

Type 1 Clamp              |  Type 2 Clamp (with left facing pipe)
:------------------------:|:-------------------------:
![](https://github.com/Aiwizo/capability/blob/master/images/clamps1.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/clamps2.png)

We are also using unsupervised learning to find images with anomalies. The algorithms are doing image inpainting, and we will use the result to train a supervised algorithm that detects certain damages.

### Object tracking

To measure the flow of traffic in intersections, we developed algorithms for tracking objects across frames. It uses [correlation filters](https://github.com/Aiwizo/capability/blob/master/object_tracking/correlation.py) for auto-correlation and cross-correlation to generalize better to new data.

There was a lack of annotated data in the domain of interest, which provided an exciting challenge. We trained the first algorithm on synthetic data that we generated by using random noise to create different textures.

To handle domain-specific edge cases better, we extract patches from videos to create background movements that are difficult to filter out.


Time Step 0             |  Time Step 1            |  Time Step 2
:------------------------:|:-------------------------:|:-------------------------:
![](https://github.com/Aiwizo/capability/blob/master/images/track0.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/track1.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/track2.png)

### Audio denoising

Currently, we are building algorithms that remove background noise in audio with speech. The training data is generated by combining audio clips containing voices from podcasts with noise. We make sure that the signal-to-noise ratio is between 0-15 (dB), and we use STFT to convert the waveforms to spectrograms.

The algorithm is a UNet with EfficientNet as its backbone. It predicts a soft mask that we multiply with the spectrogram of the mixed audio.

Mixed Spectrogram             |     Soft Mask       |  Predicted Spectrogram | Target Spectrogram |
:------------------------:|:-------------------------:|:-------------------------:|:-------------------------:
![](https://github.com/Aiwizo/capability/blob/master/images/mixed_spectrogram.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/predicted_mask.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/predicted_spectrogram.png)  |  ![](https://github.com/Aiwizo/capability/blob/master/images/target_spectrogram.png)

You can find a snapshot of our data pipeline [here](https://github.com/Aiwizo/capability/tree/master/audio_denoising/data.py)

### Identifying legal risk

One of our customers spends thousands of hours every year looking for risk in legal documents.

We did unsupervised learning to train Swedish word vectors on domain-specific documents. The legal department created a small dataset that we used to train our algorithm.

The [algorithm](https://github.com/Aiwizo/capability/blob/master/legal_risk/architecture.py) in production is a transformer-model with self-attention. Self-attention is used in the current state-of-the-art language models. We have tried using bigger models, such as BERT, but the improved performance did not outweigh the loss in speed.

We implemented several augmentations to the training data such as replacing words with similar words based on word vectors from fasttext.

### Faster annotation

We have created a novel tool called Lably to annotate data points more efficiently. The idea behind Lably is that, in large datasets, some data points are more important to annotate than others. An algorithm is continuously trained and used to select critical data points based on uncertainty.

Lably                    |
:------------------------:
![](./images/tiger-annotation.gif)

# Recreational projects

### Semi-supervised

With semi-supervised learning, we can create useful algorithms with very little data. Initially, we did mixup and mixmatch on [mnist](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mnist) with 10 annotated examples. The results were good right away, with an accuracy score of 78%. The implementation and usage are relatively simple:
- [mixup (tensorflow)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixup.py)
- [mixmatch (tensorflow, extension of mixup)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixmatch.py)
- [mixmatch (pytorch)](https://github.com/FelixAbrahamsson/mixmatch-pytorch)

### Generating climbing problems
The team at Aiwizo goes bouldering every Thursday. For fun, we developed an algorithm that generates problems on a climbing board. The chosen holds are strongly dependent on each other, which we modeled in a few different ways:

- Decoupled sampling by predicting the next hold using modified loss
- Predict full board using the Gumbel-softmax trick and modified loss for steps in-between
- Hybrid variational autoencoder with adversarial loss
- Variational autoencoder
- Generative adversarial network

### Discrete relaxation
The Gumbel-softmax trick lets us get gradients through a discrete transformation. Another recent idea is to replace the derivative of a discrete function during training and analyze what happens mathematically. This is useful for sequential modeling and for creating better variational autoencoders. 

We implemented the [original version](https://github.com/Aiwizo/capability/blob/master/discrete_relaxation/kaiser_step.py) proposed in the article, and some variants of our own, that we tried on several problems.

### Exploration by uncertainty estimation in world model
We experimented with using Bayesian model uncertainty for choosing paths to explore in reinforcement learning. We had used variational inference several times, and exploration seems like a natural place for its use. The repo is called labyrinth and can be found [here](https://github.com/samedii/labyrinth). 

We use variational inference to estimate the uncertainty of the parameters in the world model. The uncertainty is then used as the reward when learning an exploring agent. It could solve the labyrinth, but the world model was a little weak.
